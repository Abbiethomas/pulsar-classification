{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "      <th>d</th>\n",
       "      <th>e</th>\n",
       "      <th>f</th>\n",
       "      <th>g</th>\n",
       "      <th>h</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>102.507812</td>\n",
       "      <td>58.882430</td>\n",
       "      <td>0.465318</td>\n",
       "      <td>-0.515088</td>\n",
       "      <td>1.677258</td>\n",
       "      <td>14.860146</td>\n",
       "      <td>10.576487</td>\n",
       "      <td>127.393580</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>103.015625</td>\n",
       "      <td>39.341649</td>\n",
       "      <td>0.323328</td>\n",
       "      <td>1.051164</td>\n",
       "      <td>3.121237</td>\n",
       "      <td>21.744669</td>\n",
       "      <td>7.735822</td>\n",
       "      <td>63.171909</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>136.750000</td>\n",
       "      <td>57.178449</td>\n",
       "      <td>-0.068415</td>\n",
       "      <td>-0.636238</td>\n",
       "      <td>3.642977</td>\n",
       "      <td>20.959280</td>\n",
       "      <td>6.896499</td>\n",
       "      <td>53.593661</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>88.726562</td>\n",
       "      <td>40.672225</td>\n",
       "      <td>0.600866</td>\n",
       "      <td>1.123492</td>\n",
       "      <td>1.178930</td>\n",
       "      <td>11.468720</td>\n",
       "      <td>14.269573</td>\n",
       "      <td>252.567306</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>93.570312</td>\n",
       "      <td>46.698114</td>\n",
       "      <td>0.531905</td>\n",
       "      <td>0.416721</td>\n",
       "      <td>1.636288</td>\n",
       "      <td>14.545074</td>\n",
       "      <td>10.621748</td>\n",
       "      <td>131.394004</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>119.484375</td>\n",
       "      <td>48.765059</td>\n",
       "      <td>0.031460</td>\n",
       "      <td>-0.112168</td>\n",
       "      <td>0.999164</td>\n",
       "      <td>9.279612</td>\n",
       "      <td>19.206230</td>\n",
       "      <td>479.756567</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>130.382812</td>\n",
       "      <td>39.844056</td>\n",
       "      <td>-0.158323</td>\n",
       "      <td>0.389540</td>\n",
       "      <td>1.220736</td>\n",
       "      <td>14.378941</td>\n",
       "      <td>13.539456</td>\n",
       "      <td>198.236457</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>107.250000</td>\n",
       "      <td>52.627078</td>\n",
       "      <td>0.452688</td>\n",
       "      <td>0.170347</td>\n",
       "      <td>2.331940</td>\n",
       "      <td>14.486853</td>\n",
       "      <td>9.001004</td>\n",
       "      <td>107.972506</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>107.257812</td>\n",
       "      <td>39.496488</td>\n",
       "      <td>0.465882</td>\n",
       "      <td>1.162877</td>\n",
       "      <td>4.079431</td>\n",
       "      <td>24.980418</td>\n",
       "      <td>7.397080</td>\n",
       "      <td>57.784738</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>142.078125</td>\n",
       "      <td>45.288073</td>\n",
       "      <td>-0.320328</td>\n",
       "      <td>0.283953</td>\n",
       "      <td>5.376254</td>\n",
       "      <td>29.009897</td>\n",
       "      <td>6.076266</td>\n",
       "      <td>37.831393</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>133.257812</td>\n",
       "      <td>44.058244</td>\n",
       "      <td>-0.081060</td>\n",
       "      <td>0.115362</td>\n",
       "      <td>1.632107</td>\n",
       "      <td>12.007806</td>\n",
       "      <td>11.972067</td>\n",
       "      <td>195.543448</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>134.960938</td>\n",
       "      <td>49.554327</td>\n",
       "      <td>-0.135304</td>\n",
       "      <td>-0.080470</td>\n",
       "      <td>10.696488</td>\n",
       "      <td>41.342044</td>\n",
       "      <td>3.893934</td>\n",
       "      <td>14.131206</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>117.945312</td>\n",
       "      <td>45.506577</td>\n",
       "      <td>0.325438</td>\n",
       "      <td>0.661459</td>\n",
       "      <td>2.836120</td>\n",
       "      <td>23.118350</td>\n",
       "      <td>8.943212</td>\n",
       "      <td>82.475592</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>138.179688</td>\n",
       "      <td>51.524484</td>\n",
       "      <td>-0.031852</td>\n",
       "      <td>0.046797</td>\n",
       "      <td>6.330268</td>\n",
       "      <td>31.576347</td>\n",
       "      <td>5.155940</td>\n",
       "      <td>26.143310</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>114.367188</td>\n",
       "      <td>51.945716</td>\n",
       "      <td>-0.094499</td>\n",
       "      <td>-0.287984</td>\n",
       "      <td>2.738294</td>\n",
       "      <td>17.191891</td>\n",
       "      <td>9.050612</td>\n",
       "      <td>96.611903</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>109.640625</td>\n",
       "      <td>49.017652</td>\n",
       "      <td>0.137636</td>\n",
       "      <td>-0.256700</td>\n",
       "      <td>1.508361</td>\n",
       "      <td>12.072901</td>\n",
       "      <td>13.367926</td>\n",
       "      <td>223.438419</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>100.851562</td>\n",
       "      <td>51.743522</td>\n",
       "      <td>0.393837</td>\n",
       "      <td>-0.011241</td>\n",
       "      <td>2.841137</td>\n",
       "      <td>21.635778</td>\n",
       "      <td>8.302242</td>\n",
       "      <td>71.584369</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>136.093750</td>\n",
       "      <td>51.691005</td>\n",
       "      <td>-0.045909</td>\n",
       "      <td>-0.271816</td>\n",
       "      <td>9.342809</td>\n",
       "      <td>38.096400</td>\n",
       "      <td>4.345438</td>\n",
       "      <td>18.673649</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>99.367188</td>\n",
       "      <td>41.572202</td>\n",
       "      <td>1.547197</td>\n",
       "      <td>4.154106</td>\n",
       "      <td>27.555184</td>\n",
       "      <td>61.719016</td>\n",
       "      <td>2.208808</td>\n",
       "      <td>3.662680</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>100.890625</td>\n",
       "      <td>51.890394</td>\n",
       "      <td>0.627487</td>\n",
       "      <td>-0.026498</td>\n",
       "      <td>3.883779</td>\n",
       "      <td>23.045267</td>\n",
       "      <td>6.953168</td>\n",
       "      <td>52.279440</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>105.445312</td>\n",
       "      <td>41.139969</td>\n",
       "      <td>0.142654</td>\n",
       "      <td>0.320420</td>\n",
       "      <td>3.551839</td>\n",
       "      <td>20.755017</td>\n",
       "      <td>7.739552</td>\n",
       "      <td>68.519771</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>95.867188</td>\n",
       "      <td>42.059922</td>\n",
       "      <td>0.326387</td>\n",
       "      <td>0.803502</td>\n",
       "      <td>1.832776</td>\n",
       "      <td>12.248969</td>\n",
       "      <td>11.249331</td>\n",
       "      <td>177.230771</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>117.367188</td>\n",
       "      <td>53.908614</td>\n",
       "      <td>0.257953</td>\n",
       "      <td>-0.405049</td>\n",
       "      <td>6.018395</td>\n",
       "      <td>24.766123</td>\n",
       "      <td>4.807783</td>\n",
       "      <td>25.522616</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>106.648438</td>\n",
       "      <td>56.367182</td>\n",
       "      <td>0.378355</td>\n",
       "      <td>-0.266372</td>\n",
       "      <td>2.436455</td>\n",
       "      <td>18.405371</td>\n",
       "      <td>9.378660</td>\n",
       "      <td>96.860225</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>112.718750</td>\n",
       "      <td>50.301270</td>\n",
       "      <td>0.279391</td>\n",
       "      <td>-0.129011</td>\n",
       "      <td>8.281773</td>\n",
       "      <td>37.810012</td>\n",
       "      <td>4.691827</td>\n",
       "      <td>21.276210</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>130.851562</td>\n",
       "      <td>52.432857</td>\n",
       "      <td>0.142597</td>\n",
       "      <td>0.018885</td>\n",
       "      <td>2.646321</td>\n",
       "      <td>15.654436</td>\n",
       "      <td>9.464164</td>\n",
       "      <td>115.673159</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>119.437500</td>\n",
       "      <td>52.874815</td>\n",
       "      <td>-0.002549</td>\n",
       "      <td>-0.460360</td>\n",
       "      <td>2.365385</td>\n",
       "      <td>16.498032</td>\n",
       "      <td>9.008352</td>\n",
       "      <td>94.755657</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>123.210938</td>\n",
       "      <td>51.078012</td>\n",
       "      <td>0.179377</td>\n",
       "      <td>-0.177285</td>\n",
       "      <td>2.107023</td>\n",
       "      <td>16.921773</td>\n",
       "      <td>10.080333</td>\n",
       "      <td>112.558591</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>102.617188</td>\n",
       "      <td>49.692354</td>\n",
       "      <td>0.230439</td>\n",
       "      <td>0.193325</td>\n",
       "      <td>1.489130</td>\n",
       "      <td>16.004411</td>\n",
       "      <td>12.646535</td>\n",
       "      <td>171.832902</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>110.109375</td>\n",
       "      <td>41.318170</td>\n",
       "      <td>0.094860</td>\n",
       "      <td>0.683113</td>\n",
       "      <td>1.010033</td>\n",
       "      <td>13.026275</td>\n",
       "      <td>14.666511</td>\n",
       "      <td>231.204136</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17868</th>\n",
       "      <td>137.375000</td>\n",
       "      <td>50.762775</td>\n",
       "      <td>0.085075</td>\n",
       "      <td>-0.092679</td>\n",
       "      <td>7.561037</td>\n",
       "      <td>33.471080</td>\n",
       "      <td>4.782510</td>\n",
       "      <td>23.379982</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17869</th>\n",
       "      <td>110.765625</td>\n",
       "      <td>55.342186</td>\n",
       "      <td>0.066100</td>\n",
       "      <td>-0.500415</td>\n",
       "      <td>14.379599</td>\n",
       "      <td>45.320742</td>\n",
       "      <td>3.120461</td>\n",
       "      <td>8.463600</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17870</th>\n",
       "      <td>119.007812</td>\n",
       "      <td>52.091041</td>\n",
       "      <td>0.215294</td>\n",
       "      <td>-0.020229</td>\n",
       "      <td>2.012542</td>\n",
       "      <td>18.754594</td>\n",
       "      <td>10.339768</td>\n",
       "      <td>111.760522</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17871</th>\n",
       "      <td>139.898438</td>\n",
       "      <td>44.281034</td>\n",
       "      <td>-0.044618</td>\n",
       "      <td>0.100841</td>\n",
       "      <td>1.139632</td>\n",
       "      <td>11.196096</td>\n",
       "      <td>15.156427</td>\n",
       "      <td>280.553590</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17872</th>\n",
       "      <td>89.453125</td>\n",
       "      <td>35.848827</td>\n",
       "      <td>0.731656</td>\n",
       "      <td>3.101474</td>\n",
       "      <td>1.450669</td>\n",
       "      <td>14.204964</td>\n",
       "      <td>11.203558</td>\n",
       "      <td>142.473878</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17873</th>\n",
       "      <td>126.554688</td>\n",
       "      <td>54.275888</td>\n",
       "      <td>0.207825</td>\n",
       "      <td>-0.191686</td>\n",
       "      <td>2.294314</td>\n",
       "      <td>17.429850</td>\n",
       "      <td>10.469100</td>\n",
       "      <td>122.767628</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17874</th>\n",
       "      <td>133.140625</td>\n",
       "      <td>43.706181</td>\n",
       "      <td>-0.012686</td>\n",
       "      <td>0.232394</td>\n",
       "      <td>2.091973</td>\n",
       "      <td>13.489370</td>\n",
       "      <td>9.821958</td>\n",
       "      <td>131.521033</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17875</th>\n",
       "      <td>111.765625</td>\n",
       "      <td>52.943585</td>\n",
       "      <td>0.671724</td>\n",
       "      <td>0.383594</td>\n",
       "      <td>149.843646</td>\n",
       "      <td>97.926948</td>\n",
       "      <td>-0.696153</td>\n",
       "      <td>-1.260403</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17876</th>\n",
       "      <td>118.296875</td>\n",
       "      <td>49.932824</td>\n",
       "      <td>0.144401</td>\n",
       "      <td>-0.269273</td>\n",
       "      <td>1.041806</td>\n",
       "      <td>10.199265</td>\n",
       "      <td>16.739483</td>\n",
       "      <td>355.416666</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17877</th>\n",
       "      <td>90.148438</td>\n",
       "      <td>40.786044</td>\n",
       "      <td>0.457139</td>\n",
       "      <td>0.885516</td>\n",
       "      <td>5.354515</td>\n",
       "      <td>29.759511</td>\n",
       "      <td>6.102231</td>\n",
       "      <td>37.979520</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17878</th>\n",
       "      <td>107.664062</td>\n",
       "      <td>44.462463</td>\n",
       "      <td>0.296046</td>\n",
       "      <td>0.227765</td>\n",
       "      <td>6.290970</td>\n",
       "      <td>33.512873</td>\n",
       "      <td>5.536014</td>\n",
       "      <td>30.245117</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17879</th>\n",
       "      <td>105.492188</td>\n",
       "      <td>43.500718</td>\n",
       "      <td>0.154180</td>\n",
       "      <td>0.408117</td>\n",
       "      <td>2.316054</td>\n",
       "      <td>19.098507</td>\n",
       "      <td>9.377229</td>\n",
       "      <td>95.200946</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17880</th>\n",
       "      <td>130.500000</td>\n",
       "      <td>46.149553</td>\n",
       "      <td>0.084445</td>\n",
       "      <td>-0.107355</td>\n",
       "      <td>1.529264</td>\n",
       "      <td>11.757086</td>\n",
       "      <td>12.533831</td>\n",
       "      <td>211.327081</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17881</th>\n",
       "      <td>131.000000</td>\n",
       "      <td>55.935789</td>\n",
       "      <td>-0.023598</td>\n",
       "      <td>-0.536651</td>\n",
       "      <td>1.159699</td>\n",
       "      <td>10.856794</td>\n",
       "      <td>15.019974</td>\n",
       "      <td>289.362790</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17882</th>\n",
       "      <td>84.421875</td>\n",
       "      <td>41.775555</td>\n",
       "      <td>0.757467</td>\n",
       "      <td>1.107075</td>\n",
       "      <td>1.336120</td>\n",
       "      <td>12.026757</td>\n",
       "      <td>14.502863</td>\n",
       "      <td>258.119819</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17883</th>\n",
       "      <td>106.875000</td>\n",
       "      <td>47.571328</td>\n",
       "      <td>0.199440</td>\n",
       "      <td>0.284964</td>\n",
       "      <td>3.079431</td>\n",
       "      <td>20.984455</td>\n",
       "      <td>8.427475</td>\n",
       "      <td>78.259366</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17884</th>\n",
       "      <td>133.820312</td>\n",
       "      <td>43.478161</td>\n",
       "      <td>0.136691</td>\n",
       "      <td>0.353121</td>\n",
       "      <td>0.982441</td>\n",
       "      <td>9.486068</td>\n",
       "      <td>18.528395</td>\n",
       "      <td>444.411748</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17885</th>\n",
       "      <td>127.070312</td>\n",
       "      <td>47.663564</td>\n",
       "      <td>0.006552</td>\n",
       "      <td>-0.123634</td>\n",
       "      <td>0.853679</td>\n",
       "      <td>11.197206</td>\n",
       "      <td>16.122702</td>\n",
       "      <td>296.507738</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17886</th>\n",
       "      <td>121.375000</td>\n",
       "      <td>53.245158</td>\n",
       "      <td>0.103772</td>\n",
       "      <td>-0.365119</td>\n",
       "      <td>1.095318</td>\n",
       "      <td>12.239976</td>\n",
       "      <td>16.258042</td>\n",
       "      <td>303.880023</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17887</th>\n",
       "      <td>98.726562</td>\n",
       "      <td>50.407823</td>\n",
       "      <td>0.565124</td>\n",
       "      <td>0.245231</td>\n",
       "      <td>0.570234</td>\n",
       "      <td>9.011285</td>\n",
       "      <td>22.018589</td>\n",
       "      <td>561.833787</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17888</th>\n",
       "      <td>126.625000</td>\n",
       "      <td>55.721826</td>\n",
       "      <td>0.002946</td>\n",
       "      <td>-0.303218</td>\n",
       "      <td>0.534281</td>\n",
       "      <td>8.588882</td>\n",
       "      <td>23.913761</td>\n",
       "      <td>660.197035</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17889</th>\n",
       "      <td>143.671875</td>\n",
       "      <td>45.302647</td>\n",
       "      <td>-0.045769</td>\n",
       "      <td>0.353643</td>\n",
       "      <td>5.173913</td>\n",
       "      <td>26.462345</td>\n",
       "      <td>5.706651</td>\n",
       "      <td>33.802613</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17890</th>\n",
       "      <td>118.484375</td>\n",
       "      <td>50.608483</td>\n",
       "      <td>-0.029059</td>\n",
       "      <td>-0.027494</td>\n",
       "      <td>0.422241</td>\n",
       "      <td>8.086684</td>\n",
       "      <td>27.446113</td>\n",
       "      <td>830.638550</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17891</th>\n",
       "      <td>96.000000</td>\n",
       "      <td>44.193113</td>\n",
       "      <td>0.388674</td>\n",
       "      <td>0.281344</td>\n",
       "      <td>1.871237</td>\n",
       "      <td>15.833746</td>\n",
       "      <td>9.634927</td>\n",
       "      <td>104.821623</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17892</th>\n",
       "      <td>136.429688</td>\n",
       "      <td>59.847421</td>\n",
       "      <td>-0.187846</td>\n",
       "      <td>-0.738123</td>\n",
       "      <td>1.296823</td>\n",
       "      <td>12.166062</td>\n",
       "      <td>15.450260</td>\n",
       "      <td>285.931022</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17893</th>\n",
       "      <td>122.554688</td>\n",
       "      <td>49.485605</td>\n",
       "      <td>0.127978</td>\n",
       "      <td>0.323061</td>\n",
       "      <td>16.409699</td>\n",
       "      <td>44.626893</td>\n",
       "      <td>2.945244</td>\n",
       "      <td>8.297092</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17894</th>\n",
       "      <td>119.335938</td>\n",
       "      <td>59.935939</td>\n",
       "      <td>0.159363</td>\n",
       "      <td>-0.743025</td>\n",
       "      <td>21.430602</td>\n",
       "      <td>58.872000</td>\n",
       "      <td>2.499517</td>\n",
       "      <td>4.595173</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17895</th>\n",
       "      <td>114.507812</td>\n",
       "      <td>53.902400</td>\n",
       "      <td>0.201161</td>\n",
       "      <td>-0.024789</td>\n",
       "      <td>1.946488</td>\n",
       "      <td>13.381731</td>\n",
       "      <td>10.007967</td>\n",
       "      <td>134.238910</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17896</th>\n",
       "      <td>57.062500</td>\n",
       "      <td>85.797340</td>\n",
       "      <td>1.406391</td>\n",
       "      <td>0.089520</td>\n",
       "      <td>188.306020</td>\n",
       "      <td>64.712562</td>\n",
       "      <td>-1.597527</td>\n",
       "      <td>1.429475</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17897</th>\n",
       "      <td>140.562500</td>\n",
       "      <td>55.683782</td>\n",
       "      <td>-0.234571</td>\n",
       "      <td>-0.699648</td>\n",
       "      <td>3.199833</td>\n",
       "      <td>19.110426</td>\n",
       "      <td>7.975532</td>\n",
       "      <td>74.242225</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17898 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                a          b         c         d           e          f  \\\n",
       "0      102.507812  58.882430  0.465318 -0.515088    1.677258  14.860146   \n",
       "1      103.015625  39.341649  0.323328  1.051164    3.121237  21.744669   \n",
       "2      136.750000  57.178449 -0.068415 -0.636238    3.642977  20.959280   \n",
       "3       88.726562  40.672225  0.600866  1.123492    1.178930  11.468720   \n",
       "4       93.570312  46.698114  0.531905  0.416721    1.636288  14.545074   \n",
       "5      119.484375  48.765059  0.031460 -0.112168    0.999164   9.279612   \n",
       "6      130.382812  39.844056 -0.158323  0.389540    1.220736  14.378941   \n",
       "7      107.250000  52.627078  0.452688  0.170347    2.331940  14.486853   \n",
       "8      107.257812  39.496488  0.465882  1.162877    4.079431  24.980418   \n",
       "9      142.078125  45.288073 -0.320328  0.283953    5.376254  29.009897   \n",
       "10     133.257812  44.058244 -0.081060  0.115362    1.632107  12.007806   \n",
       "11     134.960938  49.554327 -0.135304 -0.080470   10.696488  41.342044   \n",
       "12     117.945312  45.506577  0.325438  0.661459    2.836120  23.118350   \n",
       "13     138.179688  51.524484 -0.031852  0.046797    6.330268  31.576347   \n",
       "14     114.367188  51.945716 -0.094499 -0.287984    2.738294  17.191891   \n",
       "15     109.640625  49.017652  0.137636 -0.256700    1.508361  12.072901   \n",
       "16     100.851562  51.743522  0.393837 -0.011241    2.841137  21.635778   \n",
       "17     136.093750  51.691005 -0.045909 -0.271816    9.342809  38.096400   \n",
       "18      99.367188  41.572202  1.547197  4.154106   27.555184  61.719016   \n",
       "19     100.890625  51.890394  0.627487 -0.026498    3.883779  23.045267   \n",
       "20     105.445312  41.139969  0.142654  0.320420    3.551839  20.755017   \n",
       "21      95.867188  42.059922  0.326387  0.803502    1.832776  12.248969   \n",
       "22     117.367188  53.908614  0.257953 -0.405049    6.018395  24.766123   \n",
       "23     106.648438  56.367182  0.378355 -0.266372    2.436455  18.405371   \n",
       "24     112.718750  50.301270  0.279391 -0.129011    8.281773  37.810012   \n",
       "25     130.851562  52.432857  0.142597  0.018885    2.646321  15.654436   \n",
       "26     119.437500  52.874815 -0.002549 -0.460360    2.365385  16.498032   \n",
       "27     123.210938  51.078012  0.179377 -0.177285    2.107023  16.921773   \n",
       "28     102.617188  49.692354  0.230439  0.193325    1.489130  16.004411   \n",
       "29     110.109375  41.318170  0.094860  0.683113    1.010033  13.026275   \n",
       "...           ...        ...       ...       ...         ...        ...   \n",
       "17868  137.375000  50.762775  0.085075 -0.092679    7.561037  33.471080   \n",
       "17869  110.765625  55.342186  0.066100 -0.500415   14.379599  45.320742   \n",
       "17870  119.007812  52.091041  0.215294 -0.020229    2.012542  18.754594   \n",
       "17871  139.898438  44.281034 -0.044618  0.100841    1.139632  11.196096   \n",
       "17872   89.453125  35.848827  0.731656  3.101474    1.450669  14.204964   \n",
       "17873  126.554688  54.275888  0.207825 -0.191686    2.294314  17.429850   \n",
       "17874  133.140625  43.706181 -0.012686  0.232394    2.091973  13.489370   \n",
       "17875  111.765625  52.943585  0.671724  0.383594  149.843646  97.926948   \n",
       "17876  118.296875  49.932824  0.144401 -0.269273    1.041806  10.199265   \n",
       "17877   90.148438  40.786044  0.457139  0.885516    5.354515  29.759511   \n",
       "17878  107.664062  44.462463  0.296046  0.227765    6.290970  33.512873   \n",
       "17879  105.492188  43.500718  0.154180  0.408117    2.316054  19.098507   \n",
       "17880  130.500000  46.149553  0.084445 -0.107355    1.529264  11.757086   \n",
       "17881  131.000000  55.935789 -0.023598 -0.536651    1.159699  10.856794   \n",
       "17882   84.421875  41.775555  0.757467  1.107075    1.336120  12.026757   \n",
       "17883  106.875000  47.571328  0.199440  0.284964    3.079431  20.984455   \n",
       "17884  133.820312  43.478161  0.136691  0.353121    0.982441   9.486068   \n",
       "17885  127.070312  47.663564  0.006552 -0.123634    0.853679  11.197206   \n",
       "17886  121.375000  53.245158  0.103772 -0.365119    1.095318  12.239976   \n",
       "17887   98.726562  50.407823  0.565124  0.245231    0.570234   9.011285   \n",
       "17888  126.625000  55.721826  0.002946 -0.303218    0.534281   8.588882   \n",
       "17889  143.671875  45.302647 -0.045769  0.353643    5.173913  26.462345   \n",
       "17890  118.484375  50.608483 -0.029059 -0.027494    0.422241   8.086684   \n",
       "17891   96.000000  44.193113  0.388674  0.281344    1.871237  15.833746   \n",
       "17892  136.429688  59.847421 -0.187846 -0.738123    1.296823  12.166062   \n",
       "17893  122.554688  49.485605  0.127978  0.323061   16.409699  44.626893   \n",
       "17894  119.335938  59.935939  0.159363 -0.743025   21.430602  58.872000   \n",
       "17895  114.507812  53.902400  0.201161 -0.024789    1.946488  13.381731   \n",
       "17896   57.062500  85.797340  1.406391  0.089520  188.306020  64.712562   \n",
       "17897  140.562500  55.683782 -0.234571 -0.699648    3.199833  19.110426   \n",
       "\n",
       "               g           h  class  \n",
       "0      10.576487  127.393580    0.0  \n",
       "1       7.735822   63.171909    0.0  \n",
       "2       6.896499   53.593661    0.0  \n",
       "3      14.269573  252.567306    0.0  \n",
       "4      10.621748  131.394004    0.0  \n",
       "5      19.206230  479.756567    0.0  \n",
       "6      13.539456  198.236457    0.0  \n",
       "7       9.001004  107.972506    0.0  \n",
       "8       7.397080   57.784738    0.0  \n",
       "9       6.076266   37.831393    0.0  \n",
       "10     11.972067  195.543448    0.0  \n",
       "11      3.893934   14.131206    0.0  \n",
       "12      8.943212   82.475592    0.0  \n",
       "13      5.155940   26.143310    0.0  \n",
       "14      9.050612   96.611903    0.0  \n",
       "15     13.367926  223.438419    0.0  \n",
       "16      8.302242   71.584369    0.0  \n",
       "17      4.345438   18.673649    0.0  \n",
       "18      2.208808    3.662680    1.0  \n",
       "19      6.953168   52.279440    0.0  \n",
       "20      7.739552   68.519771    0.0  \n",
       "21     11.249331  177.230771    0.0  \n",
       "22      4.807783   25.522616    0.0  \n",
       "23      9.378660   96.860225    0.0  \n",
       "24      4.691827   21.276210    0.0  \n",
       "25      9.464164  115.673159    0.0  \n",
       "26      9.008352   94.755657    0.0  \n",
       "27     10.080333  112.558591    0.0  \n",
       "28     12.646535  171.832902    0.0  \n",
       "29     14.666511  231.204136    0.0  \n",
       "...          ...         ...    ...  \n",
       "17868   4.782510   23.379982    0.0  \n",
       "17869   3.120461    8.463600    0.0  \n",
       "17870  10.339768  111.760522    0.0  \n",
       "17871  15.156427  280.553590    0.0  \n",
       "17872  11.203558  142.473878    0.0  \n",
       "17873  10.469100  122.767628    0.0  \n",
       "17874   9.821958  131.521033    0.0  \n",
       "17875  -0.696153   -1.260403    1.0  \n",
       "17876  16.739483  355.416666    0.0  \n",
       "17877   6.102231   37.979520    0.0  \n",
       "17878   5.536014   30.245117    0.0  \n",
       "17879   9.377229   95.200946    0.0  \n",
       "17880  12.533831  211.327081    0.0  \n",
       "17881  15.019974  289.362790    0.0  \n",
       "17882  14.502863  258.119819    0.0  \n",
       "17883   8.427475   78.259366    0.0  \n",
       "17884  18.528395  444.411748    0.0  \n",
       "17885  16.122702  296.507738    0.0  \n",
       "17886  16.258042  303.880023    0.0  \n",
       "17887  22.018589  561.833787    0.0  \n",
       "17888  23.913761  660.197035    0.0  \n",
       "17889   5.706651   33.802613    0.0  \n",
       "17890  27.446113  830.638550    0.0  \n",
       "17891   9.634927  104.821623    0.0  \n",
       "17892  15.450260  285.931022    0.0  \n",
       "17893   2.945244    8.297092    0.0  \n",
       "17894   2.499517    4.595173    0.0  \n",
       "17895  10.007967  134.238910    0.0  \n",
       "17896  -1.597527    1.429475    0.0  \n",
       "17897   7.975532   74.242225    0.0  \n",
       "\n",
       "[17898 rows x 9 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#HRTU_2 DATA !!!\n",
    "\n",
    "cluster_data=\"HTRU_2.csv\"\n",
    "df = pd.read_csv(filepath_or_buffer=cluster_data,header=0)\n",
    "df = df.rename(columns={\"140.5625\": \"a\", \"55.68378214\": \"b\",\n",
    "                       \"-0.234571412\": \"c\", \"-0.699648398\": \"d\", \n",
    "                       \"3.199832776\": \"e\", \"19.11042633\": \"f\", \n",
    "                       \"7.975531794\": \"g\", \"74.24222492\": \"h\",\n",
    "                       \"0\": \"class\"})\n",
    "df = df.append({'a': 140.5625, 'b': 55.68378214, 'c': -0.234571412, 'd': -0.699648398,\n",
    "               'e': 3.199832776, 'f': 19.11042633, 'g': 7.975531794, 'h': 74.24222492,\n",
    "               'class': 0}, ignore_index=True)\n",
    "x=df.values[:,0:-1]\n",
    "y=df.values[:,-1].ravel()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coefficients:  [[ 1.74415462e-02 -3.62372724e-02  5.85571842e+00 -5.51304158e-01\n",
      "  -2.85401409e-02  4.63597129e-02 -5.57049369e-02 -1.49577323e-03]]\n",
      "intercept:  [-6.58809904]\n",
      "accuracy 0.9793831713040563\n"
     ]
    }
   ],
   "source": [
    "#First attempt, using same data for learning as testing\n",
    "\n",
    "logistic2 = LogisticRegression(intercept_scaling=2)\n",
    "logistic2.fit(x, y)\n",
    "\n",
    "# print out the linear model\n",
    "print(\"coefficients: \", logistic2.coef_)\n",
    "print(\"intercept: \", logistic2.intercept_)\n",
    "#compute predicted values on test test;\n",
    "predicted = logistic2.predict(x);\n",
    "#print(predicted)\n",
    "print(\"accuracy\",logistic2.score(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression using gradient descent. \n",
    "#\n",
    "# input: \n",
    "#   in - training data encoded as an (n x m) matrix (1 instance per row).\n",
    "#   alpha - the learning rate, a floating point number. \n",
    "#   iterations - the number of iteratins to run the algorithm. \n",
    "#\n",
    "# output: the weights (coefficients) of the linear model.  \n",
    "#\n",
    "# For the input, assume that the target attribute is in the last column. \n",
    "def gradient_descent(indata, alpha, iterations):\n",
    "    # get the number of rows and columns in the input data. \n",
    "    # get the number of rows and columns in the input data. \n",
    "    (rows, cols) = indata.shape; \n",
    "   \n",
    "    # create a row vector of weights for each input column,\n",
    "    # initialized to 0. \n",
    "    weights = np.zeros(cols);\n",
    "    \n",
    "    # 1) \n",
    "    # Prepend  a column of 1s to the instances (for the y-intercept)\n",
    "    # 2) \n",
    "    # Separate the input attributes from the target attribute, forming\n",
    "    # two matrices.\n",
    "    instances = np.hstack((np.ones((rows,1)), indata[:,:-1]))\n",
    "    targets = indata[:,-1] \n",
    "    \n",
    "    # initialize a list to store the error at each iteration (for graphing). \n",
    "    errList = np.zeros(iterations);\n",
    "    \n",
    "    # run gradient descient for the given number of instances. \n",
    "    for i in range(iterations):\n",
    "        # Compute model for each instance: Multiply instance input matrix\n",
    "        # (n x m) by (m x 1) weight vector to get (n x 1) matrix of predictions.  \n",
    "        model = compute_model(instances,weights);\n",
    "        \n",
    "        # 5) For each instance D, compute the difference D_i between the target and the predicted value. \n",
    "        diff = (model - targets);\n",
    "        # 6) Update the weights w_1, w_2, ..., w_m. For each column position j: \n",
    "        for j in range(cols):\n",
    "          # 6.1) multiply each instance i's attribute value x_j by D_i\n",
    "          # 6.2) Sum the results of 6.1 together. \n",
    "            dJ = np.sum(diff * instances[:,j])\n",
    "          # 6.3) multiply 6.2's value by alpha and subract the result\n",
    "          # from w_j. This yields the new value for w_j\n",
    "            weights[j] = weights[j] - alpha*dJ\n",
    "        # add the count for correctly classified instances for this iteration\n",
    "        errList[i] = num_correct(model, targets)\n",
    "    return (weights, model, targets, errList)\n",
    "\n",
    "def compute_model(inst, w):\n",
    "    return 1/(1+np.exp(-1*inst.dot(w)))\n",
    "\n",
    "# count correct predictions\n",
    "def num_correct(model, targets):\n",
    "    return np.sum(classes(model) == classes(targets))\n",
    " \n",
    "# convert h(x) to a class (True or False)\n",
    "def classes(model):\n",
    "    return model >= 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights: [-2.89449525e+01  1.65022200e-01  1.29734972e-01  6.26166682e+01\n",
      " -5.40917731e-01 -4.79194632e-01 -4.24759720e-02 -4.24097715e+00\n",
      "  9.08684386e-02]\n",
      "count: 17898 correct: 17491 accuracy: 0.9772600290535255\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Hand-rolled code example from class - lower accuracy but not by much\n",
    "#Running code takes a much longer time\n",
    "\n",
    "data = df.values\n",
    "(w,predicted, target, e) = gradient_descent(data, 0.00001, 200000)\n",
    "print(\"weights:\", w)\n",
    "correct = np.sum(classes(predicted) == classes(target))\n",
    "count = len(target)\n",
    "print(\"count:\", count, \"correct:\", correct, \"accuracy:\",correct/count)\n",
    "plt.plot(e)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing any of the columns does NOT increase accuracy\n"
     ]
    }
   ],
   "source": [
    "print(\"Removing any of the columns does NOT increase accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coefficients:  [[ 0.49767746 -0.16600681  6.11719936 -3.09198337 -0.81911875  1.00244579\n",
      "   0.07118955 -0.38339555]]\n",
      "intercept:  [-4.19785536]\n",
      "accuracy 0.9789920661526428\n"
     ]
    }
   ],
   "source": [
    "#Scaling data to Logistic Regression. Doesn't improve accuracy\n",
    "\n",
    "x_scaled = scale(x)\n",
    "\n",
    "logistic3 = LogisticRegression(intercept_scaling=2)\n",
    "logistic3.fit(x_scaled, y)\n",
    "\n",
    "# print out the linear model\n",
    "print(\"coefficients: \", logistic3.coef_)\n",
    "print(\"intercept: \", logistic3.intercept_)\n",
    "#compute predicted values on test test;\n",
    "predicted3 = logistic3.predict(x_scaled);\n",
    "#print(predicted)\n",
    "print(\"accuracy\",logistic3.score(x_scaled,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coefficients:  [[ 2.53108844e-02 -3.41166021e-02  6.22899684e+00 -5.82307918e-01\n",
      "  -3.05535320e-02  5.24445612e-02 -1.09857896e-02 -2.61182933e-03]]\n",
      "intercept:  [-8.07326827]\n",
      "accuracy 0.9812849162011174\n"
     ]
    }
   ],
   "source": [
    "#Logistic regression with 80% of data set used for training and 20% used for testing\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)\n",
    "\n",
    "logistic4 = LogisticRegression(C=100.0)\n",
    "logistic4.fit(X_train, y_train)\n",
    "\n",
    "# print out the linear model\n",
    "print(\"coefficients: \", logistic4.coef_)\n",
    "print(\"intercept: \", logistic4.intercept_)\n",
    "#compute predicted values on test test;\n",
    "predicted4 = logistic4.predict(X_test);\n",
    "#print(predicted)\n",
    "print(\"accuracy\",logistic4.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.98      1.00      0.99      3264\n",
      "        1.0       0.94      0.84      0.89       316\n",
      "\n",
      "avg / total       0.98      0.98      0.98      3580\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#print(logistic4.predict_proba(X_test)[:,0:1].size)\n",
    "#print(y_test.size)\n",
    "\n",
    "#plt.scatter(X_test[:,0:1],logistic4.predict_proba(X_test)[:,0:1])\n",
    "\n",
    "print(classification_report(y_test, logistic4.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True  True  True  True  True  True  True  True]\n",
      "[1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "#Recursive Feature Elimination (RFE) is based on the idea to repeatedly construct a model and \n",
    "#choose either the best or worst performing feature, setting the feature aside and then repeating the \n",
    "#process with the rest of the features. This process is applied until all features in the dataset are exhausted. \n",
    "#The goal of RFE is to select features by recursively considering smaller and smaller sets of features.\n",
    "\n",
    "#Based on the result of all 1s, each feature is performing the same, which means that none should be given priority\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "rfe = RFE(logreg, 20)\n",
    "rfe = rfe.fit(x, y)\n",
    "print(rfe.support_)\n",
    "print(rfe.ranking_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#The receiver operating characteristic (ROC) curve is another common tool used with binary classifiers. \n",
    "#The dotted line represents the ROC curve of a purely random classifier; a good classifier stays as far \n",
    "#away from that line as possible (toward the top-left corner).\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "logit_roc_auc = roc_auc_score(y_test, logistic4.predict(X_test))\n",
    "fpr, tpr, thresholds = roc_curve(y_test, logistic4.predict_proba(X_test)[:,1])\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('Log_ROC')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3248   16]\n",
      " [  52  264]]\n"
     ]
    }
   ],
   "source": [
    "#Confustion Matrix\n",
    "#We have 3248 + 264 correct and 16 + 52 incorrect\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix = confusion_matrix(y_test, predicted4)\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data...\n",
      "Processing data...\n",
      "Preparing Linear model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing Gaussian RBF model...\n",
      "Preparing Polynomial model...\n",
      "Preparing Sigmoid model...\n",
      "Conducting predictions...\n",
      "Calculating accuracies...\n",
      "\n",
      "\n",
      "RESULTS:\n",
      "Linear model accuracy: 0.5487804878048781\n",
      "Gaussian RBF model accuracy: 0.9984756097560976\n",
      "Polynomical model accuracy: 0.725609756097561\n",
      "Sigmoid model accuracy: 0.42378048780487804\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\"\"\"\n",
    "Read in data\n",
    "\"\"\"\n",
    "print(\"Reading data...\")\n",
    "# df = pd.read_csv('HTRU_2.csv')\n",
    "df0 = pd.read_csv('data0.csv')\n",
    "df1 = pd.read_csv('data1.csv')\n",
    "\n",
    "df0 = df0.sample(n=len(df1), random_state = 0)\n",
    "\n",
    "\"\"\"\n",
    "Preprocess and data preparation\n",
    "\"\"\"\n",
    "print(\"Processing data...\")\n",
    "\n",
    "pre_features_0 = df0.iloc[: , :8]\n",
    "pre_target_0 = df0['class']\n",
    "\n",
    "# rescale variables to have 0 mean and unit variance\n",
    "df_columns_0 = pre_features_0.columns\n",
    "scaler = StandardScaler()\n",
    "scaledf_0 = scaler.fit_transform(pre_features_0)\n",
    "\n",
    "pre_features_1 = df1.iloc[: , :8]\n",
    "pre_target_1 = df1['class']\n",
    "\n",
    "# rescale variables to have 0 mean and unit variance\n",
    "df_columns_1 = pre_features_1.columns\n",
    "scaledf_1 = scaler.fit_transform(pre_features_1)\n",
    "\n",
    "# restructure dataframe\n",
    "df0 = pd.DataFrame(scaledf_0)\n",
    "df0.columns = df_columns_0\n",
    "df0['class'] = pre_target_0\n",
    "df0['class'] = df0['class'].fillna(0).astype(np.int64)\n",
    "\n",
    "df1 = pd.DataFrame(scaledf_1)\n",
    "df1.columns = df_columns_1\n",
    "df1['class'] = pre_target_1\n",
    "\n",
    "# reassign feature and targets to newly scaled variables\n",
    "features_0 = df0.iloc[:, :8]\n",
    "target_0 = df0['class']\n",
    "\n",
    "features_1 = df1.iloc[:, :8]\n",
    "target_1 = df1['class']\n",
    "\n",
    "# pre_features = df.iloc[: , :8]\n",
    "# pre_target = df['class']\n",
    "\n",
    "# # rescale variables to have 0 mean and unit variance\n",
    "# df_columns = pre_features.columns\n",
    "# scaler = StandardScaler()\n",
    "# scaledf = scaler.fit_transform(pre_features)\n",
    "\n",
    "# # restructure dataframe\n",
    "# df = pd.DataFrame(scaledf)\n",
    "# df.columns = df_columns\n",
    "# df['class'] = pre_target\n",
    "\n",
    "# # reassign feature and targets to newly scaled variables\n",
    "# features = df.iloc[:, :8]\n",
    "# target = df['class']\n",
    "\n",
    "# split dataset into test and train split\n",
    "# 80/20 split\n",
    "feat_train_0, feat_test_0, target_train_0, target_test_0 = train_test_split(features_0, target_0, train_size = 0.8, random_state = 0)\n",
    "feat_train_1, feat_test_1, target_train_1, target_test_1 = train_test_split(features_1, target_1, train_size = 0.8, random_state = 0)\n",
    "\n",
    "feat_train = feat_train_0.append(feat_train_1)\n",
    "feat_test = feat_test_0.append(feat_test_1)\n",
    "target_train = target_train_0.append(target_train_1)\n",
    "target_test = target_test_0.append(target_test_1)\n",
    "\n",
    "# print(feat_train)\n",
    "# print(feat_test)\n",
    "# print(target_train)\n",
    "# print(target_test)\n",
    "\n",
    "\"\"\"\n",
    "Set up different SVM models to see which performs best.\n",
    "\n",
    "C is set to 1 to start and if different hyperplanes are needed, C is raised or \n",
    "lowered accordingly.\n",
    "\n",
    "For Gaussian model, we set gamma to 1 to begin. If we need less or more curvature\n",
    "in our decision boundary, we will adjust gamma.\n",
    "\n",
    "We use 'ovr' as our decision function shape. This stands for one vs rest or \n",
    "one vs all. Since we only have two classes 0 or 1, this will work fine for our\n",
    "purposes.\n",
    "\"\"\"\n",
    "# Linear\n",
    "print(\"Preparing Linear model...\")\n",
    "linearSVM = svm.SVC(kernel='linear', C=1, decision_function_shape = 'ovo')\n",
    "linearSVM.fit(feat_train, target_train)\n",
    "\n",
    "# Gaussian RBF\n",
    "print(\"Preparing Gaussian RBF model...\")\n",
    "rbfSVM = svm.SVC(kernel='rbf', gamma=1, C=1, decision_function_shape = 'ovr')\n",
    "rbfSVM.fit(feat_train, target_train)\n",
    "\n",
    "# Polynomial\n",
    "print(\"Preparing Polynomial model...\")\n",
    "polySVM = svm.SVC(kernel='poly', degree=3, C=1, decision_function_shape = 'ovo')\n",
    "polySVM.fit(feat_train, target_train)\n",
    "\n",
    "# Sigmoid\n",
    "print(\"Preparing Sigmoid model...\")\n",
    "sigSVM = svm.SVC(kernel='sigmoid', C=1, decision_function_shape='ovo')\n",
    "sigSVM.fit(feat_train, target_train)\n",
    "\n",
    "# Predict using the SVM models\n",
    "print(\"Conducting predictions...\")\n",
    "linear_pred = linearSVM.predict(feat_test)\n",
    "rbf_pred = rbfSVM.predict(feat_test)\n",
    "poly_pred = polySVM.predict(feat_test)\n",
    "sig_pred = sigSVM.predict(feat_test)\n",
    "\n",
    "# Calculate the accuracies of each model\n",
    "print(\"Calculating accuracies...\")\n",
    "linear_acc = accuracy_score(linear_pred, target_test)\n",
    "rbf_acc = accuracy_score(rbf_pred, target_test)\n",
    "poly_acc = accuracy_score(poly_pred, target_test)\n",
    "sig_acc = accuracy_score(sig_pred, target_test)\n",
    "\n",
    "# Print results\n",
    "print(\"\\n\\nRESULTS:\")\n",
    "print(\"Linear model accuracy:\", linear_acc)\n",
    "print(\"Gaussian RBF model accuracy:\", rbf_acc)\n",
    "print(\"Polynomical model accuracy:\", poly_acc)\n",
    "print(\"Sigmoid model accuracy:\", sig_acc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coefficients:  [[ 0.01792469  0.07980263  0.0570014   0.0465818  -0.042674    0.01542118\n",
      "  -0.06962592  0.06473762]]\n",
      "intercept:  [3.34600541e-05]\n",
      "accuracy 0.4801829268292683\n"
     ]
    }
   ],
   "source": [
    "#Logistic regression with 20% of data set used for training and 80% used for testing\n",
    "\n",
    "\n",
    "logistic5 = LogisticRegression()\n",
    "logistic5.fit(feat_train, target_train)\n",
    "\n",
    "# print out the linear model\n",
    "print(\"coefficients: \", logistic5.coef_)\n",
    "print(\"intercept: \", logistic5.intercept_)\n",
    "#compute predicted values on test test;\n",
    "predicted5 = logistic5.predict(feat_test);\n",
    "#print(predicted)\n",
    "print(\"accuracy\",logistic5.score(feat_test, target_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
